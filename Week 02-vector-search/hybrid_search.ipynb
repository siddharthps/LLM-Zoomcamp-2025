{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cd69ee",
   "metadata": {},
   "source": [
    "# Hybrid search with Qdrant\n",
    "\n",
    "Vector search based on dense embeddings captures the semantics of the data, so you don't have to use the same terms in queries and documents to still be able to find the relevant items. However, historically we were also using some other methods which rely on the presence of the keywords. Methods such as Bag-of-Words, TFIDF and BM25 are still useful and in some cases should be preferred over the dense embeddings. \n",
    "\n",
    "## Sparse vectors\n",
    "\n",
    "Surprisingly, keyword-based search is also implemented as vector search, but these vectors are usually sparse. That means the majority of the dimensions of such a vector are just zeros. A non-zero value at a particular vector dimension indicates the presence of a term from the dictionary assigned to that position. In other words, in sparse vectors, we have a dictionary in which each word/phrase gets its unique position. Since vectors are sparse, the dictionary can theoretically grow indefinitely, as we can append a new term at the very end. \n",
    "\n",
    "The fact of using a flexible dictionary, make the sparse vectors excel in exact matches, as they can cover texts that would be sets of random characters for the dense vectors - such as proper names or identifiers. Dense embedding models also have a dictionary, but once the model is trained, extending them is not that easy, and requires fine-tuning of the model. A typical user rarely goes that far.\n",
    "\n",
    "### BM25\n",
    "\n",
    "There are plenty of different options for creating sparse embeddings, but BM25 is an industry standard, and its most popular form comes from the 90s. It's a statistical model (no neural networks involved), which makes it really fast and lightweight. It's usually a solid baseline in search benchmarks so you should not ignore it.\n",
    "\n",
    "BM25 stands for Best Matching 25, and it was just the 25th attempt to create a formula that calculates how relevant a particular document is, given a query. If you are interested in mathematical background, please check out the [Wikipedia page](https://en.wikipedia.org/wiki/Okapi_BM25) that describes it really well. In general, BM25 is a ranking function that helps search engines determine how relevant a document is to a query by combining two key concepts: **Term Frequency (TF)** and **Inverse Document Frequency (IDF)**. \n",
    "\n",
    "1. The Term Frequency component rewards documents that contain the query terms multiple times, but with diminishing returns - so a document with 10 occurrences of a word isn't necessarily 10 times better than one with just 1 occurrence. \n",
    "2. The Inverse Document Frequency part boosts the importance of rare words while reducing the weight of common words that appear in many documents, since rare terms are typically more informative for distinguishing relevant results. \n",
    "\n",
    "BM25 also incorporates document length normalization to prevent longer documents from having an unfair advantage simply due to their size.\n",
    "\n",
    "In our case, we'll use an implementation available in FastEmbed. Let's start with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17127ee0",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "\n",
    "Please refer to [sematic_search.ipynb](sematic_search.ipynb) notebook to set up the libraries required to interact with Qdrant and to create the embeddings. Similarly, please start Qdrant in a Docker container as described there, if it's not running on your machine yet. \n",
    "\n",
    "If you skipped our previous lessons, the following commands will install all the packages and run Qdrant in a background container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q \"qdrant-client[fastembed]>=1.14.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 6333:6333 -p 6334:6334 \\\n",
    "   -v \"./qdrant_storage:/qdrant/storage:z\" \\\n",
    "   qdrant/qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b32ff5",
   "metadata": {},
   "source": [
    "## Step 1: Connect to Qdrant\n",
    "\n",
    "Let's connect to Qdrant and test if that was successful by listing all the collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf62ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='zoomcamp-faq'), CollectionDescription(name='zoomcamp-rag')])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525ae48",
   "metadata": {},
   "source": [
    "## Step 2: Sparse vector search with BM25\n",
    "\n",
    "We are going to use the same dataset as before. Let's download it and load into Qdrant, but this time we are going to create sparse vectors with BM25 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeac8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9efb9",
   "metadata": {},
   "source": [
    "We need to create a collection first. Qdrant will handle the IDF calculations, if we configure it to. That's required for BM25, otherwise it won't boost the rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c5a4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Create the collection with specified sparse vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3171a",
   "metadata": {},
   "source": [
    "FastEmbed comes with a BM25 implementation that we can use as any other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb45281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952eb0d196de4505a79d40e944aa7f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeb95baf490428198ba5e2f95d27d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "arabic.txt:   0%|          | 0.00/6.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth\\Downloads\\LLM Zoomcamp - 2025\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Siddharth\\AppData\\Local\\Temp\\fastembed_cache\\models--Qdrant--bm25. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2abb81033ab446dad8ac6e64ae590ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a074656915749a296891d6bc7643769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed89db38777249d3bc61d57958c5d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9627d0e08a2a4b09a1aa98a48ac46bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finnish.txt:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfaef2fd0ff49bcbbfe9f1c50c2a4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7aafa1bd1d1437085d4de24a63cdef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8763700039f40b0876a1d363dd73bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german.txt:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7c28454ce644e9b1420ab1bcd07c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "greek.txt:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e86a02d10904375acf9f1e56ef86362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian.txt:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea642dfbf3ac4c4b957a6c8066ed19e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hungarian.txt:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af37e8a3405940e792a1f3a042a284c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "portuguese.txt:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6545be7f6f964d43b755f1b9ba7d93d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "russian.txt:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9194943a337406e9f1face1408cff6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spanish.txt:   0%|          | 0.00/2.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8de11e00e83413baba9dd2c256384af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "romanian.txt:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653cb1fbfce241498bf0460c17b6ea31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8faa5720144fa196c118b6dafbdb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eea00ffd42541dcb0bc7551bb1c40f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Send the points to the collection\n",
    "client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b4d86",
   "metadata": {},
   "source": [
    "You might be surprised how fast the upload operation was. BM25 does not require a neural network, so it is fast compared to dense embedding models.\n",
    "\n",
    "## Step 3: Running sparse vector search with BM25\n",
    "\n",
    "Right now, our vectors are ready to be searched over. Let's create a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "949bd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse\",\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\",\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7ac1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search(\"Qdrant\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1301b61",
   "metadata": {},
   "source": [
    "Sparse vectors can return no results, if none of the keywords from the query were ever used in the documents. No matter if there are some synonyms. Terminology does matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8347afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n"
     ]
    }
   ],
   "source": [
    "results = search(\"pandas\")\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097d9c5",
   "metadata": {},
   "source": [
    "Scores returned by BM25 are not calculated with cosine similarity, but with BM25 formula. They are not bounded to a specific range, but are virtually unbounded. Let's see how they may look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c252b3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0392046"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7933a1",
   "metadata": {},
   "source": [
    "That's an important observation before we start implementing hybrid search.\n",
    "\n",
    "### Natural language like queries\n",
    "\n",
    "Let's try again with a random question from our dataset to see how well sparse vector search can work with longer, natural language like queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b71aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(202506)\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course[\"documents\"])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65cd0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\n",
      "Detailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\n",
      "Source code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py\n"
     ]
    }
   ],
   "source": [
    "results = search(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445977a9",
   "metadata": {},
   "source": [
    "### Step 4: Qdrant Universal Query API - prefetching\n",
    "\n",
    "Qdrant's `.query_points` method allows building multi-step search pipelines which can incorporate various methods into a single call. For example, we can retrieve some candidates with dense vector search, and then rerank them with sparse search, or use a fast method for initial retrieval and precise, but slow, reranking.\n",
    "\n",
    "```ascii\n",
    "┌─────────────┐           ┌─────────────┐\n",
    "│             │           │             │\n",
    "│  Retrieval  │ ────────► │  Reranking  │\n",
    "│             │           │             │\n",
    "└─────────────┘           └─────────────┘\n",
    "```\n",
    "\n",
    "Let's create another collection that will keep both dense and sparse representations. Qdrant named vectors allow us to store multiple representations per point and it proves useful especially when we want to use mulitple models in our applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a153b239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection with both vector types\n",
    "client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    vectors_config={\n",
    "        # Named dense vector for jinaai/jina-embeddings-v2-small-en\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1ebe4",
   "metadata": {},
   "source": [
    "We have to upload all the vectors into the newly created collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bcff2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf16f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                # Prefetch ten times more results, then\n",
    "                # expected to return, so we can really rerank\n",
    "                limit=(10 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\", \n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79320aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b43d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem description. How can we connect s3 bucket to MLFLOW?\n",
      "Solution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\n",
      "Read more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
      "Added by Akshit Miglani\n"
     ]
    }
   ],
   "source": [
    "results = multi_stage_search(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0af186",
   "metadata": {},
   "source": [
    "## Step 5: Building Hybrid Search\n",
    "\n",
    "In real production systems, you don't need to choose just one vector type. You never know what kind of queries your users will send to the system. E-commerce search might be just fine with lexical search on top of sparse vectors, as people will tend to send keywords, but in conversational systems, such as chatbots, natural language questions might be more frequent. Using one model as a retriever and another one as reranker is not the only way of how to use dense and sparse in a single system.\n",
    "\n",
    "Hybrid Search is a technique for combining results coming from different search methods - for example dense and sparse. There isn't a clear definition of how exactly to implement it, as the main problem is how to mix results coming from methods which are incompatible. Dense and sparse search scores can't be compared directly, so we need another method that will order the final results somehow.\n",
    "\n",
    "There are two terms important for Hybrid Search: **fusion** and **reranking**.\n",
    "\n",
    "### Fusion\n",
    "\n",
    "Fusion is a set of methods which work on the scores/ranking as returned by the individual methods. There are various ways of how to achieve that, but Reciprocal Rank Fusion is the most popular technique. It is based on the rankings of the documents in each methods used, and these rankings are used to calculate the final scores. You will never calculate these scores, as Qdrant has some built-in capabilities that we will use. However, the following example can give you a rough intuition:\n",
    "\n",
    "| Document | Dense ranking | Sparse ranking | RRF score | Final ranking |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| D1 | **1** | 5 | 0.0318 | 2 |\n",
    "| D2 | 2 | 4 | 0.0317 | 3 |\n",
    "| D3 | 3 | 2 | 0.0320 | **1** |\n",
    "| D4 | 4 | 3 | 0.0315 | 5 |\n",
    "| D5 | 5 | **1** | 0.0318 | 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7de1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Fusion query enables fusion on the prefetched results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e82ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\"\n",
      "}\n",
      "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\n",
      "Detailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\n",
      "Source code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py\n"
     ]
    }
   ],
   "source": [
    "results = rrf_search(course_piece[\"question\"])\n",
    "print(json.dumps(course_piece, indent=2))\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645833e",
   "metadata": {},
   "source": [
    "### Reranking\n",
    "\n",
    "Reranking is a broader term related to Hybrid Search. Fusion is one of the ways to rerank the results of multiple methods, but you can also apply a slower method that won't be effective enough to search over all the documents. But there is more to it. Business rules are often important for retrieval, as you prefer to show documents coming from the most recent news, for instance.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Dense and sparse vector search methods might not be enough in some cases, but both are fast enough to be used as initial retrievers. Plenty of more accurate yet slower methods exist, such as cross-encoders or [multivector representations](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/). These topics are definitely more advanced, and we won't cover them right now. However, it's good to mention them so you are aware they exist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
